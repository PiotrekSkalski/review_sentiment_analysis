{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from get_dataset import get_dataset\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.data as data\n",
    "from torch.optim import Adam\n",
    "from torchtext.data import Iterator\n",
    "from importlib import reload\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have dark theme\n",
    "from matplotlib.pyplot import rcParams\n",
    "rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "import logging\n",
    "logging.basicConfig(filename='log.txt',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s : %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, emb_weights = get_dataset()\n",
    "dataset.fields['review'].include_lengths = True\n",
    "\n",
    "random.seed(43)\n",
    "ds_train, ds_val, ds_test = dataset.split(split_ratio=[0.8, 0.1, 0.1], random_state=random.getstate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator\n",
    "from torchtext.data import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.366\n"
     ]
    }
   ],
   "source": [
    "padding_list = []\n",
    "for i in range(1, 11):\n",
    "    it = Iterator(dataset, 8, shuffle=True)\n",
    "    for batch in it:\n",
    "        lengths = batch.review[1]\n",
    "        max_length = lengths.max().item()\n",
    "        padding_sum = (max_length - lengths).tolist()\n",
    "        padding_list.extend(padding_sum)\n",
    "avg_pad_length = sum(padding_list)/len(padding_list)\n",
    "print(avg_pad_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.366\n"
     ]
    }
   ],
   "source": [
    "padding_list = []\n",
    "for i in range(1, 11):\n",
    "    it = BucketIterator(dataset, 8, shuffle=True, sort_key=lambda x: len(x.review))\n",
    "    for batch in it:\n",
    "        lengths = batch.review[1]\n",
    "        max_length = lengths.max().item()\n",
    "        padding_sum = (max_length - lengths).tolist()\n",
    "        padding_list.extend(padding_sum)\n",
    "avg_pad_length = sum(padding_list)/len(padding_list)\n",
    "print(avg_pad_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 50):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7153333333333334\n"
     ]
    }
   ],
   "source": [
    "padding_list = []\n",
    "for i in range(1, 11):\n",
    "    it = MyIterator(dataset, 8, shuffle=True, train=True, sort_key=lambda x: len(x.review))\n",
    "    for batch in it:\n",
    "        lengths = batch.review[1]\n",
    "        max_length = lengths.max().item()\n",
    "        padding_sum = (max_length - lengths).tolist()\n",
    "        padding_list.extend(padding_sum)\n",
    "avg_pad_length = sum(padding_list)/len(padding_list)\n",
    "print(avg_pad_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, query_dim, n_outputs, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(query_dim, query_dim//2)\n",
    "        self.W2 = nn.Linear(query_dim//2, n_outputs)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query):\n",
    "        attn_weights = self.W2(self.dropout(torch.tanh(self.W1(query))))\n",
    "        attn_weights = attn_weights.permute(0,2,1)\n",
    "        \n",
    "        return self.softmax(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_SelfAttention_model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, embed_vecs=None, hidden_size=512,\n",
    "                num_layers=1, attn_ouput_size=8, dropout=(0, 0), bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_output_size = attn_ouput_size\n",
    "        if embed_vecs is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embed_vecs)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(input_size=embed_dim, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, dropout=dropout[1],\n",
    "                          bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout[0])\n",
    "        self.attention = SelfAttention(self.num_directions*self.hidden_size, attn_ouput_size, dropout[0])\n",
    "        self.head = nn.Linear(self.attn_output_size*self.num_directions*self.hidden_size, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch, lengths = batch\n",
    "        batch_dim, _ = batch.shape\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(batch))\n",
    "        embedded_packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        outputs_packed, hiddens = self.gru(embedded_packed)\n",
    "        \n",
    "        outputs, lengths = nn.utils.rnn.pad_packed_sequence(outputs_packed, batch_first=True)\n",
    "#         last_hidden = hiddens.view(self.num_layers, self.num_directions, batch_dim, self.hidden_size)[-1,:,:,:]\n",
    "#         hidden_concat = last_hidden.transpose(1,0).reshape(batch_dim, self.num_directions*self.hidden_size)\n",
    "        \n",
    "        attn_weights = self.attention(self.dropout(outputs))\n",
    "        attn_output = torch.bmm(attn_weights, outputs).view(batch_dim, -1)\n",
    "        \n",
    "        logging.debug('batch shape : {}'.format(batch.shape))\n",
    "        logging.debug('embedding shape : {}'.format(embedded.shape))\n",
    "        logging.debug('hiddens shape : {}'.format(hiddens.shape))\n",
    "        logging.debug('outputs shape : {}'.format(outputs.shape))\n",
    "#         logging.debug('hidden_concat shape : {}'.format(hidden_concat.shape))\n",
    "        logging.debug('attn_weights shape : {}'.format(attn_weights.shape))\n",
    "        logging.debug('attn_output shape : {}'.format(attn_output.shape))\n",
    "        \n",
    "        return self.head(self.dropout(attn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def validate(ds, loss_fn, model, bs=1, device=device):\n",
    "    \"\"\"\n",
    "        Loops over a dataset (validation or test) and evaluates average\n",
    "        loss and accuracy of a given model.\n",
    "    \"\"\"\n",
    "    is_in_train = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "#         size = len(ds)\n",
    "        predictions = []\n",
    "        gt = []\n",
    "        loss = 0\n",
    "        for i, batch in enumerate(MyIterator(ds, bs, sort_key=lambda x: len(x.review), shuffle=False, train=False, device=device)):\n",
    "            output = model(batch.review)\n",
    "            predictions.extend(output.argmax(dim=1).tolist())\n",
    "            gt.extend(batch.label.tolist())\n",
    "            loss += loss_fn(output, batch.label).item()\n",
    "        avg_loss = loss/(i+1)\n",
    "\n",
    "    accuracy = np.mean(np.array(predictions) == np.array(gt))\n",
    "    if is_in_train: model.train()\n",
    "        \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(model, loss_fn, optimiser, epochs=1, bs=4, device=device, grad_clip=None):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(MyIterator(ds_train, bs, sort_key=lambda x: len(x.review), shuffle=True, device=device), 1):\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            output = model(batch.review)\n",
    "            loss = loss_fn(output, batch.label)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            optimiser.step()\n",
    "\n",
    "            if not i % (len(ds_train)//(bs*3)):\n",
    "                avg_loss = total_loss / (len(ds_train)//(bs*3))\n",
    "                val_loss, val_accuracy = validate(ds_val, loss_fn, model, bs=bs)\n",
    "                print('Epoch : {}, batch : {}, train_loss = {:.4f}, val_loss = {:.4f}, val_accuracy : {:.3f}, time = {:.0f}s'.format(\n",
    "                        epoch + 1, i, avg_loss, val_loss, val_accuracy, time.time() - start_time))\n",
    "                total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(emb_weights)\n",
    "embed_size = 300\n",
    "\n",
    "model = GRU_SelfAttention_model(vocab_size, embed_size, emb_weights.clone(), bidirectional=True,\n",
    "                                num_layers=2, hidden_size=32, attn_ouput_size=8, dropout=(0.4, 0.5)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, batch : 100, train_loss = 0.6843, val_loss = 0.6942, val_accuracy : 0.480, time = 4s\n",
      "Epoch : 1, batch : 200, train_loss = 0.6477, val_loss = 0.6067, val_accuracy : 0.657, time = 8s\n",
      "Epoch : 1, batch : 300, train_loss = 0.6081, val_loss = 0.5375, val_accuracy : 0.750, time = 13s\n",
      "Epoch : 2, batch : 100, train_loss = 0.5271, val_loss = 0.4929, val_accuracy : 0.750, time = 18s\n",
      "Epoch : 2, batch : 200, train_loss = 0.4800, val_loss = 0.4609, val_accuracy : 0.783, time = 23s\n",
      "Epoch : 2, batch : 300, train_loss = 0.4774, val_loss = 0.4398, val_accuracy : 0.790, time = 28s\n",
      "Epoch : 3, batch : 100, train_loss = 0.4549, val_loss = 0.4245, val_accuracy : 0.817, time = 33s\n",
      "Epoch : 3, batch : 200, train_loss = 0.4189, val_loss = 0.4035, val_accuracy : 0.803, time = 37s\n",
      "Epoch : 3, batch : 300, train_loss = 0.4326, val_loss = 0.3917, val_accuracy : 0.833, time = 42s\n",
      "Epoch : 4, batch : 100, train_loss = 0.4296, val_loss = 0.4257, val_accuracy : 0.823, time = 46s\n",
      "Epoch : 4, batch : 200, train_loss = 0.4095, val_loss = 0.3684, val_accuracy : 0.850, time = 51s\n",
      "Epoch : 4, batch : 300, train_loss = 0.4201, val_loss = 0.3650, val_accuracy : 0.843, time = 55s\n",
      "Epoch : 5, batch : 100, train_loss = 0.4171, val_loss = 0.3687, val_accuracy : 0.840, time = 60s\n",
      "Epoch : 5, batch : 200, train_loss = 0.3897, val_loss = 0.3431, val_accuracy : 0.863, time = 65s\n",
      "Epoch : 5, batch : 300, train_loss = 0.4092, val_loss = 0.3777, val_accuracy : 0.840, time = 69s\n",
      "Epoch : 6, batch : 100, train_loss = 0.3879, val_loss = 0.3646, val_accuracy : 0.840, time = 74s\n",
      "Epoch : 6, batch : 200, train_loss = 0.3499, val_loss = 0.3342, val_accuracy : 0.870, time = 78s\n",
      "Epoch : 6, batch : 300, train_loss = 0.3900, val_loss = 0.3533, val_accuracy : 0.857, time = 83s\n",
      "Epoch : 7, batch : 100, train_loss = 0.3840, val_loss = 0.3654, val_accuracy : 0.853, time = 88s\n",
      "Epoch : 7, batch : 200, train_loss = 0.3523, val_loss = 0.3316, val_accuracy : 0.863, time = 92s\n",
      "Epoch : 7, batch : 300, train_loss = 0.3701, val_loss = 0.3323, val_accuracy : 0.870, time = 96s\n",
      "Epoch : 8, batch : 100, train_loss = 0.3670, val_loss = 0.3484, val_accuracy : 0.867, time = 101s\n",
      "Epoch : 8, batch : 200, train_loss = 0.3547, val_loss = 0.3217, val_accuracy : 0.887, time = 105s\n",
      "Epoch : 8, batch : 300, train_loss = 0.3468, val_loss = 0.3197, val_accuracy : 0.880, time = 110s\n"
     ]
    }
   ],
   "source": [
    "optimiser = Adam(model.parameters(), lr=3e-4)\n",
    "learner(model, loss_fn, optimiser, epochs=8, bs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, batch : 100, train_loss = 0.6961, val_loss = 0.6725, val_accuracy : 0.647, time = 4s\n",
      "Epoch : 1, batch : 200, train_loss = 0.6602, val_loss = 0.6044, val_accuracy : 0.720, time = 9s\n",
      "Epoch : 1, batch : 300, train_loss = 0.5909, val_loss = 0.5619, val_accuracy : 0.697, time = 15s\n",
      "Epoch : 2, batch : 100, train_loss = 0.5560, val_loss = 0.4787, val_accuracy : 0.777, time = 21s\n",
      "Epoch : 2, batch : 200, train_loss = 0.4776, val_loss = 0.4868, val_accuracy : 0.773, time = 27s\n",
      "Epoch : 2, batch : 300, train_loss = 0.4775, val_loss = 0.4192, val_accuracy : 0.813, time = 33s\n",
      "Epoch : 3, batch : 100, train_loss = 0.4469, val_loss = 0.4023, val_accuracy : 0.830, time = 38s\n",
      "Epoch : 3, batch : 200, train_loss = 0.4192, val_loss = 0.4156, val_accuracy : 0.823, time = 44s\n",
      "Epoch : 3, batch : 300, train_loss = 0.4293, val_loss = 0.3928, val_accuracy : 0.837, time = 49s\n",
      "Epoch : 4, batch : 100, train_loss = 0.4257, val_loss = 0.3824, val_accuracy : 0.847, time = 55s\n",
      "Epoch : 4, batch : 200, train_loss = 0.4084, val_loss = 0.3721, val_accuracy : 0.860, time = 61s\n",
      "Epoch : 4, batch : 300, train_loss = 0.4299, val_loss = 0.3710, val_accuracy : 0.853, time = 66s\n",
      "Epoch : 5, batch : 100, train_loss = 0.3766, val_loss = 0.3609, val_accuracy : 0.867, time = 73s\n",
      "Epoch : 5, batch : 200, train_loss = 0.3764, val_loss = 0.3543, val_accuracy : 0.873, time = 79s\n",
      "Epoch : 5, batch : 300, train_loss = 0.4032, val_loss = 0.3504, val_accuracy : 0.863, time = 84s\n",
      "Epoch : 6, batch : 100, train_loss = 0.4053, val_loss = 0.3735, val_accuracy : 0.840, time = 90s\n",
      "Epoch : 6, batch : 200, train_loss = 0.3672, val_loss = 0.3484, val_accuracy : 0.873, time = 96s\n",
      "Epoch : 6, batch : 300, train_loss = 0.3680, val_loss = 0.3370, val_accuracy : 0.870, time = 102s\n",
      "Epoch : 7, batch : 100, train_loss = 0.3818, val_loss = 0.3634, val_accuracy : 0.850, time = 108s\n",
      "Epoch : 7, batch : 200, train_loss = 0.3282, val_loss = 0.3423, val_accuracy : 0.883, time = 114s\n",
      "Epoch : 7, batch : 300, train_loss = 0.3753, val_loss = 0.3284, val_accuracy : 0.867, time = 121s\n",
      "Epoch : 8, batch : 100, train_loss = 0.3480, val_loss = 0.3663, val_accuracy : 0.843, time = 127s\n",
      "Epoch : 8, batch : 200, train_loss = 0.3028, val_loss = 0.3331, val_accuracy : 0.890, time = 133s\n",
      "Epoch : 8, batch : 300, train_loss = 0.3601, val_loss = 0.3338, val_accuracy : 0.870, time = 139s\n"
     ]
    }
   ],
   "source": [
    "optimiser = Adam(model.parameters(), lr=3e-4)\n",
    "learner(model, loss_fn, optimiser, epochs=8, bs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, batch : 100, train_loss = 0.3509, val_loss = 0.3299, val_accuracy : 0.867, time = 4s\n",
      "Epoch : 1, batch : 200, train_loss = 0.3282, val_loss = 0.3186, val_accuracy : 0.880, time = 9s\n",
      "Epoch : 1, batch : 300, train_loss = 0.3177, val_loss = 0.3191, val_accuracy : 0.880, time = 15s\n",
      "Epoch : 2, batch : 100, train_loss = 0.3431, val_loss = 0.3286, val_accuracy : 0.870, time = 22s\n",
      "Epoch : 2, batch : 200, train_loss = 0.3039, val_loss = 0.3195, val_accuracy : 0.880, time = 28s\n",
      "Epoch : 2, batch : 300, train_loss = 0.3219, val_loss = 0.3211, val_accuracy : 0.890, time = 33s\n",
      "Epoch : 3, batch : 100, train_loss = 0.3441, val_loss = 0.3246, val_accuracy : 0.870, time = 39s\n",
      "Epoch : 3, batch : 200, train_loss = 0.2867, val_loss = 0.3207, val_accuracy : 0.880, time = 45s\n",
      "Epoch : 3, batch : 300, train_loss = 0.3514, val_loss = 0.3184, val_accuracy : 0.890, time = 51s\n"
     ]
    }
   ],
   "source": [
    "optimiser.param_groups[0]['lr'] = 1e-4\n",
    "learner(model, loss_fn, optimiser, epochs=3, bs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.26914, test accuracy : 0.883\n"
     ]
    }
   ],
   "source": [
    "print('Test loss : {:.5f}, test accuracy : {:.03f}'.format(*validate(ds_test, loss_fn, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, batch : 100, train_loss = 0.3011, val_loss = 0.3228, val_accuracy : 0.887, time = 7s\n",
      "Epoch : 1, batch : 200, train_loss = 0.2996, val_loss = 0.3191, val_accuracy : 0.880, time = 14s\n",
      "Epoch : 1, batch : 300, train_loss = 0.3170, val_loss = 0.3170, val_accuracy : 0.893, time = 21s\n",
      "Epoch : 2, batch : 100, train_loss = 0.3007, val_loss = 0.3294, val_accuracy : 0.870, time = 29s\n",
      "Epoch : 2, batch : 200, train_loss = 0.2744, val_loss = 0.3178, val_accuracy : 0.883, time = 37s\n",
      "Epoch : 2, batch : 300, train_loss = 0.2973, val_loss = 0.3161, val_accuracy : 0.887, time = 44s\n",
      "Epoch : 3, batch : 100, train_loss = 0.2928, val_loss = 0.3282, val_accuracy : 0.863, time = 52s\n",
      "Epoch : 3, batch : 200, train_loss = 0.2599, val_loss = 0.3168, val_accuracy : 0.887, time = 60s\n",
      "Epoch : 3, batch : 300, train_loss = 0.2856, val_loss = 0.3117, val_accuracy : 0.890, time = 67s\n",
      "Epoch : 4, batch : 100, train_loss = 0.2620, val_loss = 0.3298, val_accuracy : 0.863, time = 75s\n",
      "Epoch : 4, batch : 200, train_loss = 0.2479, val_loss = 0.3189, val_accuracy : 0.890, time = 83s\n",
      "Epoch : 4, batch : 300, train_loss = 0.2790, val_loss = 0.3139, val_accuracy : 0.897, time = 90s\n",
      "Epoch : 5, batch : 100, train_loss = 0.2622, val_loss = 0.3309, val_accuracy : 0.870, time = 98s\n",
      "Epoch : 5, batch : 200, train_loss = 0.2297, val_loss = 0.3159, val_accuracy : 0.893, time = 106s\n",
      "Epoch : 5, batch : 300, train_loss = 0.2744, val_loss = 0.3156, val_accuracy : 0.890, time = 113s\n"
     ]
    }
   ],
   "source": [
    "model.embedding.weight.requires_grad_(True);\n",
    "optimiser.param_groups[0]['lr'] = 1e-4\n",
    "learner(model, loss_fn, optimiser, epochs=5, bs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.26950, test accuracy : 0.900\n"
     ]
    }
   ],
   "source": [
    "print('Test loss : {:.5f}, test accuracy : {:.03f}'.format(*utils.validate(ds_test, loss_fn, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, batch : 100, train_loss = 0.2598, val_loss = 0.3224, val_accuracy : 0.893, time = 8s\n",
      "Epoch : 1, batch : 200, train_loss = 0.2140, val_loss = 0.3202, val_accuracy : 0.890, time = 15s\n",
      "Epoch : 1, batch : 300, train_loss = 0.2579, val_loss = 0.3179, val_accuracy : 0.893, time = 24s\n",
      "Epoch : 2, batch : 100, train_loss = 0.2600, val_loss = 0.3229, val_accuracy : 0.890, time = 32s\n",
      "Epoch : 2, batch : 200, train_loss = 0.2218, val_loss = 0.3190, val_accuracy : 0.887, time = 40s\n",
      "Epoch : 2, batch : 300, train_loss = 0.2383, val_loss = 0.3162, val_accuracy : 0.890, time = 48s\n",
      "Epoch : 3, batch : 100, train_loss = 0.2382, val_loss = 0.3197, val_accuracy : 0.880, time = 56s\n",
      "Epoch : 3, batch : 200, train_loss = 0.1969, val_loss = 0.3192, val_accuracy : 0.890, time = 64s\n",
      "Epoch : 3, batch : 300, train_loss = 0.2505, val_loss = 0.3161, val_accuracy : 0.893, time = 72s\n",
      "Epoch : 4, batch : 100, train_loss = 0.2452, val_loss = 0.3199, val_accuracy : 0.887, time = 79s\n",
      "Epoch : 4, batch : 200, train_loss = 0.1974, val_loss = 0.3200, val_accuracy : 0.890, time = 87s\n",
      "Epoch : 4, batch : 300, train_loss = 0.2419, val_loss = 0.3184, val_accuracy : 0.890, time = 95s\n",
      "Epoch : 5, batch : 100, train_loss = 0.2333, val_loss = 0.3267, val_accuracy : 0.880, time = 103s\n",
      "Epoch : 5, batch : 200, train_loss = 0.1978, val_loss = 0.3221, val_accuracy : 0.890, time = 111s\n",
      "Epoch : 5, batch : 300, train_loss = 0.2404, val_loss = 0.3204, val_accuracy : 0.890, time = 118s\n"
     ]
    }
   ],
   "source": [
    "model.embedding.weight.requires_grad_(True);\n",
    "optimiser.param_groups[0]['lr'] = 5e-5\n",
    "learner(model, loss_fn, optimiser, epochs=5, bs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.27463, test accuracy : 0.897\n"
     ]
    }
   ],
   "source": [
    "print('Test loss : {:.5f}, test accuracy : {:.03f}'.format(*utils.validate(ds_test, loss_fn, model)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
